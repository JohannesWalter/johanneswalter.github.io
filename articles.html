<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <!-- <title>Reviewed Articles - [Your Name]</title> -->
  <link rel="stylesheet" href="styles.css" />
  <!-- Font Awesome Icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css">
</head>
<body>
  <!-- Navigation Bar -->
  <header>
    <div class="container">
      <!-- Your Name -->
      <div class="header-top">
        <!-- <h1>Johannes Walter</h1> -->
      </div>
      <!-- Navigation Bar -->
      <nav class="navbar">
        <ul class="nav-links">
          <li><a href="index.html">Home</a></li>
          <li><a href="articles.html">Research</a></li>
          <!-- <li><a href="policy-media.html">Policy & Media Contributions</a></li> -->
          <li><a href="CV.pdf">CV</a></li>
        </ul>
        <a href="#" id="darkModeToggle" title="Toggle Dark Mode">
          <i class="fas fa-lightbulb"></i>
          <span class="mode-text">Dark Mode</span>
        </a>
      </nav>
    </div>
  </header>

  <!-- Main Content -->
  <main class="container">
    
    <ul class="paper-list">
      <li class="paper-item">
        <h3 class="paper-title">Using AI Persuasion to Reduce Political Polarization</h3>
        <p class="paper-meta">Job Market Paper</p>
        <p class="paper-abstract">AI chatbots can reduce overall polarization across different topics, compared to
          different controls, with effects persisting after one month. AI chatbots perform
          on par with incentivized humans and static text interventions, but uniquely improve
          some measures for affective polarization, enjoyment, and individualization.
          Through two pre‐registered randomized controlled trials with representative
          samples of the U.S. population (N=811 and N=838), I provide the first comprehensive
          experimental evidence that AI‐powered conversational agents can effectively
          reduce political polarization on contentious issues. The first experiment demonstrated
          that an AI chatbot successfully persuaded participants to adopt more moderate
          views on U.S. support for Ukraine, reducing overall ideological polarization
          by approximately 20 percentage points. The second experiment compared AI persuasion
          to human persuaders and static text on immigration policy. All three interventions
          significantly reduced participants' distance from moderate positions
          by about 10 percentage points, with no statistically significant differences in persuasive
          effectiveness between treatments. However, participants rated AI conversations
          as significantly more enjoyable and felt their individual concerns were
          better addressed by the AI compared to other interventions. Affective polarization
          showed limited improvements across all treatments. These findings demonstrate
          that AI‐powered persuasion could serve as a cheap, scalable tool for reducing political
          polarization while highlighting important concerns about potential misuse
          by political parties and geo‐political adversaries, underscoring the need for careful
          regulation of AI persuasion technologies.</p>
        <div class="paper-links">
          <a href="Using-AI-Persuasion-to-Reduce-Political-Polarization.pdf" target="_blank">Paper</a>
        </div>
      </li>

      <li class="paper-item">
        <h3 class="paper-title">Advised by an Algorithm: Learning with Different Informational Resources and Reactions to Heterogeneous Advice Quality</h3>
        <p class="paper-meta">Joint work with Jan Biermann and John Horton</p>
        <p class="paper-abstract">In a wide range of settings, decision-makers increasingly rely on algorithmic tools for
            support. Often, the algorithm serves as an advisor, leaving the final decision to be made
            by human judgment. In this setting, we focus on two aspects: first, identifying the informational
            resources that aid individuals in evaluating algorithmic guidance, and second,
            exploring human reactions to varying qualities of algorithmic advice. To address these
            questions, we conducted an online experiment involving 1565 participants. In the baseline
            treatment, subjects repeatedly perform the same estimation task and are provided with
            algorithmic guidance, all without knowledge of the type of algorithm or feedback after
            each round. Subsequently, we introduce two interventions aimed at enhancing the quality
            of human decisions when receiving algorithmic advice. In the first intervention, we explain
            the way the algorithm functions. We find that while this intervention reduces adherence
            to algorithmic advice, it does not improve decision-making performance. In the second
            treatment, we disclose the correct answer to the task after each round. This intervention
            leads to a reduction in adherence to algorithmic advice and an improvement in human
            decision-making performance. Furthermore, we investigate the extent to which individuals
            can adjust their assessment of the algorithm when advice quality fluctuates due to
            external circumstances. We find some evidence that individuals can assess algorithmic
            advice thoughtfully, adjusting their adherence depending on the quality of algorithmic
            recommendations.</p>
        <div class="paper-links">
          <a href="Advised-by-an-Algorithm.pdf" target="_blank">Paper</a>
        </div>
      </li>

      <!-- <li>
        <a href="article2.pdf" target="_blank">The Effect of AI on the demand for Human Expertise</a>
        <p>Joint work with Sebastian Valet.</p>
        <br>
        <p>This paper investigates the impact of consumer AI adoption, specifically ChatGPT, 
          on the demand for human expertise. Utilizing a dual methodology, we first analyze extensive 
          observational data from over 100,000 users to assess the downstream effects of ChatGPT adoption. 
          Our findings indicate a significant reduction in visits to websites offering human expertise, 
          such as WebMD and Quora, following AI adoption. In the second part of our study, we conduct an 
          online lab experiment to explore the underlying mechanisms. Participants are tasked with finding 
          information about a disease they currently have, with three groups: one interacting with an AI, 
          another using online search, and a control group with no assistance. The outcome measured is the 
          self-reported likelihood of visiting a doctor. Results show that participants in the AI treatment 
          group report a significantly lower probability of seeking medical advice compared to the online 
          search and control groups. These findings suggest that AI expertise may crowd out human knowledge, 
          with profound implications for regulation and the labor market. </p>
        <br>
        <div class="links">
          <a href="article2.pdf" target="_blank">[paper]</a>
          <a href="code2.zip" target="_blank">[code]</a>
          <a href="data2.zip" target="_blank">[data]</a>
          <a href="preregistration2.pdf" target="_blank">[preregistration]</a>
          <a href="ethical_approval2.pdf" target="_blank">[ethical approval]</a>
        </div>
      </li> -->

      <li class="paper-item">
        <h3 class="paper-title">Testing Novelty Incentives in Human Red Teaming</h3>
        <p class="paper-meta">Joint work with Dominik Rehse and Sebastian Valet</p>
        <p class="paper-abstract">We test whether paying for novel failures makes human red‐teaming more efficient. In
          a real‐time market, each model reply is scored for harassment and for novelty (from
          embeddings). Two pre‐registered Prolific experiments pit a harm‐only control against
          a treatment paid for novelty‐weighted harm under two regimes: in Experiment 1, treatment
          bonuses can be at most equal to control; in Experiment 2, they are at least equal.
          This two‐regime design intentionally separates pay/risk effects from the novelty objective
          itself. Novelty incentives push search into new areas and raise novelty but make eliciting
          harassment harder. Efficiency improves under the first regime (more novelty‐weighted
          harm per euro) but not under the second, where higher pay fails to lift efficiency. On average,
          treatment yields lower novelty‐weighted harm as novelty gains are offset by lower
          harassment. Ex‐post, treatment inputs are more diverse and semantically distinct; outputs
          show no consistent diversity gains.</p>
        <div class="paper-links">
          <a href="Testing-Novelty-Incentives-in-Human-Red-Teaming.pdf" target="_blank">Paper</a>
        </div>
      </li>
      <!-- <br><br /><br><br />
      <li>
        <a href="article2.pdf" target="_blank">Incentives and Economics of Data Sharing</a>
        <p>Joint work with Dominik Rehse and Sebastian Valet.</p>
        <br>
        <p>Abstract: ??</p>
        <br>
        <div class="links">
          <a href="article2.pdf" target="_blank">[paper]</a>
          <a href="code2.zip" target="_blank">[code]</a>
          <a href="data2.zip" target="_blank">[data]</a>
          <a href="preregistration2.pdf" target="_blank">[preregistration]</a>
          <a href="ethical_approval2.pdf" target="_blank">[ethical approval]</a>
        </div>
      </li> -->
      <!-- Add more articles as needed -->
    </ul>
  </main>

  <!-- Footer -->
  <footer>
    <div class="container">
        <p>&copy; 2025 Johannes Walter. Content available under 
            <a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener noreferrer">
                CC BY-NC 4.0
            </a> unless otherwise noted.
        </p>
    </div>
</footer>

<script src="script.js"></script>
</body>
</html> 